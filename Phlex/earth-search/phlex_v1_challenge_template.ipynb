import os
import pandas as pd
import numpy as np
import rioxarray
import geopandas as gpd
from sklearn.metrics import f1_score
from datetime import datetime
import torch
import yaml
import math
from torchvision import transforms as v2
from torch.utils.data import DataLoader
from rasterio.enums import Resampling
from tqdm import tqdm
import pystac_client
import stackstac

from src.model import ClayMAEModule  # Replace with your model path

# Create a variable 'eligibility' to mark prize eligibility
eligibility = 1  # Change to 0 if the submission is only ranking eligible

# Initialize Earth Engine (if needed)
# ee.Initialize()

# Load model and metadata
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
ckpt = "https://clay-model-ckpt.s3.amazonaws.com/v0.5.7/mae_v0.5.7_epoch-13_val-loss-0.3098.ckpt"
torch.set_default_device(device)
torch.cuda.empty_cache()  # Clear GPU cache

model = ClayMAEModule.load_from_checkpoint(
    ckpt, metadata_path="configs/metadata.yaml", shuffle=False, mask_ratio=0
)
model.eval()
model = model.to(device)

metadata = Box(yaml.safe_load(open("configs/metadata.yaml")))

# Functions for generating embeddings
def normalize_timestamp(date):
    week = date.isocalendar().week * 2 * np.pi / 52
    hour = date.hour * 2 * np.pi / 24
    return (math.sin(week), math.cos(week)), (math.sin(hour), math.cos(hour))

def normalize_latlon(lat, lon):
    lat = lat * np.pi / 180
    lon = lon * np.pi / 180
    return (math.sin(lat), math.cos(lat)), (math.sin(lon), math.cos(lon))

def to_device(data, device):
    if isinstance(data, torch.Tensor):
        return data.to(device)
    elif isinstance(data, dict):
        return {k: to_device(v, device) for k, v in data.items()}
    elif isinstance(data, list):
        return [to_device(v, device) for v in data]
    return data

def process_point(lon, lat, model, metadata, start_date, end_date, COLLECTION, device):
    if COLLECTION == "landsat-c2-l2":
        os.environ['AWS_REQUEST_PAYER'] = 'requester'
    model.to(device)  # Ensure the model is on the correct device
    catalog = pystac_client.Client.open("https://earth-search.aws.element84.com/v1")
    search = catalog.search(
        collections=[COLLECTION],
        datetime=f"{start_date}/{end_date}",
        bbox=(lon - 1e-5, lat - 1e-5, lon + 1e-5, lat + 1e-5),
        max_items=10,
        query={"eo:cloud_cover": {"lt": 80}},
    )

    all_items = search.get_all_items()
    items = list(all_items)
    if not items:
        return None

    items = sorted(items, key=lambda x: x.properties.get('eo:cloud_cover', float('inf')))
    lowest_cloud_item = items[0]

    epsg = lowest_cloud_item.properties["proj:epsg"]

    poidf = gpd.GeoDataFrame(
        pd.DataFrame(),
        crs="EPSG:4326",
        geometry=[Point(lon, lat)],
    ).to_crs(epsg)

    coords = poidf.iloc[0].geometry.coords[0]

    size = 256
    gsd = 10 if COLLECTION == "sentinel-2-l2a" else 30
    bounds = (
        coords[0] - (size * gsd) // 2,
        coords[1] - (size * gsd) // 2,
        coords[0] + (size * gsd) // 2,
        coords[1] + (size * gsd) // 2,
    )

    stack = stackstac.stack(
        lowest_cloud_item,
        bounds=bounds,
        snap_bounds=False,
        epsg=epsg,
        resolution=gsd,
        dtype="float64",
        rescale=False,
        fill_value=0.0,
        assets=["blue", "green", "red", "nir"],
        resampling=Resampling.nearest,
    )

    stack = stack.compute()

    items = []
    dates = []
    for item in all_items:
        if item.datetime.date() not in dates:
            items.append(item)
            dates.append(item.datetime.date())

    platform = "sentinel-2-l2a" if COLLECTION == "sentinel-2-l2a" else "landsat-c2l1"
    mean = []
    std = []
    waves = []
    for band in stack.band:
        mean.append(metadata[platform].bands.mean[str(band.values)])
        std.append(metadata[platform].bands.std[str(band.values)])
        waves.append(metadata[platform].bands.wavelength[str(band.values)])

    transform = v2.Compose([v2.Normalize(mean=mean, std=std)])

    datetimes = stack.time.values.astype("datetime64[s]").tolist()
    times = [normalize_timestamp(dat) for dat in datetimes]
    week_norm = [dat[0] for dat in times]
    hour_norm = [dat[1] for dat in times]

    latlons = [normalize_latlon(lat, lon)] * len(times)
    lat_norm = [dat[0] for dat in latlons]
    lon_norm = [dat[1] for dat in latlons]

    pixels = torch.from_numpy(stack.data.astype(np.float32)).to(device)
    pixels = transform(pixels)

    batch_size = 16
    num_batches = math.ceil(len(stack) / batch_size)
    
    embeddings_list = []
    for i in range(num_batches):
        start_idx = i * batch_size
        end_idx = min((i + 1) * batch_size, len(stack))
        
        batch_pixels = pixels[start_idx:end_idx].to(device)
        batch_time = torch.tensor(np.hstack((week_norm, hour_norm))[start_idx:end_idx], dtype=torch.float32).to(device)
        batch_latlon = torch.tensor(np.hstack((lat_norm, lon_norm))[start_idx:end_idx], dtype=torch.float32).to(device)
        
        batch_datacube = {
            "platform": platform,
            "time": batch_time,
            "latlon": batch_latlon,
            "pixels": batch_pixels,
            "gsd": torch.tensor(stack.gsd.values).to(device),
            "waves": torch.tensor(waves).to(device),
        }

        batch_datacube = to_device(batch_datacube, device)

        try:
            model = model.to(device)

            with torch.no_grad():
                unmsk_patch, _, _, _ = model.model.encoder(batch_datacube)
            batch_embeddings = unmsk_patch[:, 0, :].cpu().numpy()
            embeddings_list.append(batch_embeddings)
        except RuntimeError as e:
            if "out of memory" in str(e):
                print(f"GPU OOM for point ({lon}, {lat}), batch {i+1}/{num_batches}. Trying CPU...")
                device = torch.device("cpu")
                batch_datacube = to_device(batch_datacube, device)
                model = model.to(device)
                with torch.no_grad():
                    unmsk_patch, _, _, _ = model.model.encoder(batch_datacube)
                batch_embeddings = unmsk_patch[:, 0, :].numpy()
                embeddings_list.append(batch_embeddings)
            else:
                raise e

    embeddings = np.concatenate(embeddings_list, axis=0)
    return embeddings

def create_embeddings(start_date, end_date, grid, COLLECTION, start_date2=None, end_date2=None):
    torch.cuda.empty_cache()  # Clear GPU cache
    
    # Assuming grid is a GeoDataFrame with points
    points = grid.to_crs("EPSG:4326").geometry.apply(lambda x: (x.x, x.y)).tolist()

    # Storing results in a list
    results = []

    # Iterate over points and process each
    for i, point in enumerate(tqdm(points)):
        lon, lat = point
        embeddings = process_point(lon, lat, model, metadata, start_date, end_date, COLLECTION, device="cuda")
        
        # Initialize embeddings_new as None
        embeddings_new = None
        if start_date2 is not None and end_date2 is not None:
            embeddings_new = process_point(lon, lat, model, metadata, start_date2, end_date2, COLLECTION, device="cuda")
        
        if embeddings is not None:
            if embeddings_new is not None:
                results.append((lon, lat, embeddings, embeddings_new))
            else:
                results.append((lon, lat, embeddings))

    # Define columns based on the presence of embeddings_new
    columns = ["lon", "lat", "embeddings"]
    if start_date2 is not None and end_date2 is not None:
        columns.append("embeddings_new")

    # Create DataFrame from results
    df = pd.DataFrame(results, columns=columns)

    # Convert to GeoDataFrame
    gdf_results = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon, df.lat))

    return gdf_results

def get_flood_value_at_point(geometry):
    try:
        clipped = FLOOD_RASTER.rio.clip([geometry], FLOOD_RASTER.rio.crs)
        return float(clipped.mean().values)
    except rioxarray.exceptions.NoDataInBounds as e:
        return np.nan

def NN_preds(task_id, model_type, gdf):
    # Example function to get predictions from a model
    # Implement this function according to your requirements
    gdf['pred'] = np.random.random(len(gdf))  # Placeholder, replace with actual logic
    return gdf

def process_geometries(gdf, func):
    results = []
    for _, row in gdf.iterrows():
        result = func(row['geometry'])
        results.append(result)
    return results

def root_mean_squared_error(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

# Main code for task 5
COLLECTION = "sentinel-2-l2a"

# Load GeoJSON file and generate image grid
geojson_path = 'test_data/challenge_5_bb.geojson'
grid = gdf_preprocess(geojson_path, COLLECTION=COLLECTION)

# Create embeddings for the given date ranges
gdf = create_embeddings(start_date="2018-08-01", end_date="2018-09-30", 
                        start_date2="2019-08-01", end_date2="2019-09-30", 
                        grid=grid, COLLECTION=COLLECTION)
gdf['embeddings_delta'] = gdf['embeddings_new'] - gdf['embeddings']

# Replace model with code for predictions
gdf = NN_preds(task_id=5, model_type="binary_classifier", gdf=gdf)

# Load test values
gdf['geometry'] = grid.geometry
test_values = process_geometries(gdf, get_flood_value_at_point)

# Calculate and record the result
submission = {}
submission['task_5_score'] = f1_score(test_values, gdf["pred"])

# Save submission
submission_path = 'results/submission_task_5.json'
with open(submission_path, 'w') as f:
    json.dump(submission, f)

print(f"Task 5 Score: {submission['task_5_score']:.4f}")
