import os
import pandas as pd
import numpy as np
import geopandas as gpd
from sklearn.metrics import f1_score
import torch
import yaml
from rasterio.enums import Resampling
from torch.utils.data import DataLoader
from tqdm import tqdm
import pystac_client
import stackstac

from src.model import ClayMAEModule 

eligibility = 1  # Change to 0 if the submission is only ranking eligible

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

ckpt = "https://clay-model-ckpt.s3.amazonaws.com/v0.5.7/mae_v0.5.7_epoch-13_val-loss-0.3098.ckpt"
model = ClayMAEModule.load_from_checkpoint(ckpt, metadata_path="configs/metadata.yaml", shuffle=False, mask_ratio=0)
model.eval()
model = model.to(device)

with open("configs/metadata.yaml") as file:
    metadata = yaml.safe_load(file)

def normalize_timestamp(date):
    week = date.isocalendar().week * 2 * np.pi / 52
    hour = date.hour * 2 * np.pi / 24
    return (np.sin(week), np.cos(week)), (np.sin(hour), np.cos(hour))

def normalize_latlon(lat, lon):
    lat = lat * np.pi / 180
    lon = lon * np.pi / 180
    return (np.sin(lat), np.cos(lat)), (np.sin(lon), np.cos(lon))

def process_point(lon, lat, model, metadata, start_date, end_date, COLLECTION):
    model.to(device)
    catalog = pystac_client.Client.open("https://earth-search.aws.element84.com/v1")
    
    
    search = catalog.search(
        collections=[COLLECTION],
        datetime=f"{start_date}/{end_date}",
        bbox=(lon - 1e-5, lat - 1e-5, lon + 1e-5, lat + 1e-5),
        max_items=10,
        query={"eo:cloud_cover": {"lt": 80}},
    )
    
    items = list(search.get_all_items())
    if not items:
        return None

    lowest_cloud_item = sorted(items, key=lambda x: x.properties.get('eo:cloud_cover', float('inf')))[0]
    epsg = lowest_cloud_item.properties["proj:epsg"]

    
    poidf = gpd.GeoDataFrame(geometry=[gpd.points_from_xy([lon], [lat])], crs="EPSG:4326").to_crs(epsg)
    coords = poidf.iloc[0].geometry.coords[0]
    
    
    stack = stackstac.stack(
        lowest_cloud_item, bounds=(coords[0] - 128, coords[1] - 128, coords[0] + 128, coords[1] + 128),
        epsg=epsg, resolution=10, assets=["B02", "B03", "B04", "B08"], resampling=Resampling.nearest
    )
    
    stack = stack.compute()
    pixels = torch.from_numpy(stack.data).float().to(device)
    
    with torch.no_grad():
        embeddings = model(pixels.unsqueeze(0))  
    return embeddings.cpu().numpy()

def create_embeddings(grid, start_date, end_date, COLLECTION="sentinel-2-l2a"):
    results = []
    for _, row in tqdm(grid.iterrows(), total=len(grid)):
        lon, lat = row.geometry.x, row.geometry.y
        embeddings = process_point(lon, lat, model, metadata, start_date, end_date, COLLECTION)
        if embeddings is not None:
            results.append((lon, lat, embeddings))
    return pd.DataFrame(results, columns=["Longitude", "Latitude", "Embeddings"])

# GeoJSON 
geojson_path = 'path_to_your_geojson.geojson'
grid = gpd.read_file(geojson_path)

start_date_1 = "2018-08-01"
end_date_1 = "2018-09-30"
start_date_2 = "2019-08-01"
end_date_2 = "2019-09-30"

gdf_1 = create_embeddings(grid, start_date_1, end_date_1)
gdf_2 = create_embeddings(grid, start_date_2, end_date_2)

gdf_1['Embeddings_Delta'] = gdf_2['Embeddings'] - gdf_1['Embeddings']

gdf_1['Predictions'] = np.random.randint(0, 2, len(gdf_1))  # Это заглушка, замените на ваши предсказания

gdf_1['Test_Values'] = np.random.randint(0, 2, len(gdf_1))  # Это заглушка, замените на реальные тестовые значения

# F1
f1 = f1_score(gdf_1['Test_Values'], gdf_1['Predictions'])
print(f"F1 Score: {f1:.4f}")

gdf_1.to_csv('task_5_results.csv', index=False)
